{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, RobertaForSequenceClassification\n",
    "from datasets import load_dataset, ClassLabel, Value, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load source dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_us_reviews (/home/zh2095/.cache/huggingface/datasets/amazon_us_reviews/Video_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563)\n"
     ]
    }
   ],
   "source": [
    "# sampling the dataset for fine-tuning\n",
    "train = load_dataset('amazon_us_reviews', 'Video_v1_00', split='train[:60%]') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'marketplace': 'US',\n",
       " 'customer_id': '49033728',\n",
       " 'review_id': 'R1P1G5KZ05H6RD',\n",
       " 'product_id': '6302503213',\n",
       " 'product_parent': '748506413',\n",
       " 'product_title': 'The Night They Saved Christmas [VHS]',\n",
       " 'product_category': 'Video',\n",
       " 'star_rating': 5,\n",
       " 'helpful_votes': 0,\n",
       " 'total_votes': 0,\n",
       " 'vine': 0,\n",
       " 'verified_purchase': 1,\n",
       " 'review_headline': 'Very satisfied!!',\n",
       " 'review_body': 'Fast shipping. Pleasure to deal with. Would recommend. A+++. Thanks!',\n",
       " 'review_date': '2015-08-31'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at a sample\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unuseful columns\n",
    "train = train.remove_columns(['marketplace', 'review_id', 'product_parent', 'product_title', 'product_category', \\\n",
    "                      'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'customer_id': '49033728',\n",
       " 'product_id': '6302503213',\n",
       " 'star_rating': 5,\n",
       " 'review_headline': 'Very satisfied!!',\n",
       " 'review_body': 'Fast shipping. Pleasure to deal with. Would recommend. A+++. Thanks!'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer pretrained on roberta-base\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7ea7c0c0b64cf2bfde4a0595152705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228362 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encode the training dataset in the form of sentences pair\n",
    "# truncate at length=32 for a balance of time consuming and information coverage\n",
    "train_tokenized = train.map(lambda batch: tokenizer(batch['review_body'], padding='max_length', truncation=True, max_length=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = train_tokenized.rename_column(\"star_rating\", \"labels\")\n",
    "train_tokenized = train_tokenized.rename_column(\"review_body\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0951c5e0cae449e486ddc676702b7a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228362 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert star rating that ranging from 1-5 to labels that ranging from 0-4\n",
    "def to_label(x):\n",
    "    x['labels']  = x['labels'] - 1\n",
    "    return x\n",
    "\n",
    "train_tokenized = train_tokenized.map(to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized.set_format(\"torch\", columns=[\"input_ids\",  \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(4),\n",
       " 'input_ids': tensor([    0, 35515,  6738,     4, 18486, 24669,     7,   432,    19,     4,\n",
       "         11258,  5940,     4,    83, 42964, 27079,  4557,   328,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zh2095/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/zh2095/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load a pretrained model\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# set training arguments manually if needed, otherwise use the defalut\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=0,                # number of warmup steps for learning rate scheduler\n",
    "    learning_rate=5e-5,               # learning rate\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Trainer object with the model, training arguments, training and test datasets, and evaluation function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args = training_args,\n",
    "    train_dataset=train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up gpu cache before training\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: product_id, review_headline, text, customer_id. If product_id, review_headline, text, customer_id are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/ext3/conda/bootcamp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 228362\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10707\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10707' max='10707' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10707/10707 35:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.856600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.829600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.807900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.750200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.733400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.727100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-500\n",
      "Configuration saved in ./output/checkpoint-500/config.json\n",
      "Model weights saved in ./output/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-1000\n",
      "Configuration saved in ./output/checkpoint-1000/config.json\n",
      "Model weights saved in ./output/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-1500\n",
      "Configuration saved in ./output/checkpoint-1500/config.json\n",
      "Model weights saved in ./output/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-2000\n",
      "Configuration saved in ./output/checkpoint-2000/config.json\n",
      "Model weights saved in ./output/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-2500\n",
      "Configuration saved in ./output/checkpoint-2500/config.json\n",
      "Model weights saved in ./output/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-3000\n",
      "Configuration saved in ./output/checkpoint-3000/config.json\n",
      "Model weights saved in ./output/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-3500\n",
      "Configuration saved in ./output/checkpoint-3500/config.json\n",
      "Model weights saved in ./output/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-4000\n",
      "Configuration saved in ./output/checkpoint-4000/config.json\n",
      "Model weights saved in ./output/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-4500\n",
      "Configuration saved in ./output/checkpoint-4500/config.json\n",
      "Model weights saved in ./output/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-5000\n",
      "Configuration saved in ./output/checkpoint-5000/config.json\n",
      "Model weights saved in ./output/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-5500\n",
      "Configuration saved in ./output/checkpoint-5500/config.json\n",
      "Model weights saved in ./output/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-6000\n",
      "Configuration saved in ./output/checkpoint-6000/config.json\n",
      "Model weights saved in ./output/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-6500\n",
      "Configuration saved in ./output/checkpoint-6500/config.json\n",
      "Model weights saved in ./output/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-7000\n",
      "Configuration saved in ./output/checkpoint-7000/config.json\n",
      "Model weights saved in ./output/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-7500\n",
      "Configuration saved in ./output/checkpoint-7500/config.json\n",
      "Model weights saved in ./output/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-8000\n",
      "Configuration saved in ./output/checkpoint-8000/config.json\n",
      "Model weights saved in ./output/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-8500\n",
      "Configuration saved in ./output/checkpoint-8500/config.json\n",
      "Model weights saved in ./output/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-9000\n",
      "Configuration saved in ./output/checkpoint-9000/config.json\n",
      "Model weights saved in ./output/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-9500\n",
      "Configuration saved in ./output/checkpoint-9500/config.json\n",
      "Model weights saved in ./output/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-10000\n",
      "Configuration saved in ./output/checkpoint-10000/config.json\n",
      "Model weights saved in ./output/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-10500\n",
      "Configuration saved in ./output/checkpoint-10500/config.json\n",
      "Model weights saved in ./output/checkpoint-10500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10707, training_loss=0.822625502692091, metrics={'train_runtime': 2146.8103, 'train_samples_per_second': 319.118, 'train_steps_per_second': 4.987, 'total_flos': 1.1266159449997312e+16, 'train_loss': 0.822625502692091, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()    \n",
    "# trainer.train(resume_from_checkpoint=True) # True if already trained, to save time by continuing on a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in Roberta-senti/config.json\n",
      "Model weights saved in Roberta-senti/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# save the fine_tuned model\n",
    "\n",
    "model.save_pretrained(\"Roberta-senti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on the whole dataset with the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_us_reviews (/home/zh2095/.cache/huggingface/datasets/amazon_us_reviews/Video_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563)\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/zh2095/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/zh2095/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/zh2095/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d105e279817487cab0bf05dcded29d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/380604 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e570c7943eaf4624bc0307a06130aa4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/380604 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the entire dataset\n",
    "dataset = load_dataset('amazon_us_reviews', 'Video_v1_00', split='train')\n",
    "\n",
    "# remove unuseful columns\n",
    "dataset = dataset.remove_columns(['marketplace', 'review_id', 'product_parent', 'product_title', 'product_category', \\\n",
    "                      'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_date'])\n",
    "    \n",
    "# load the fine-tuned tokenizer  \n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# encode the training dataset in the form of sentences pair\n",
    "# truncate at length=32 for a balance of time consuming and information coverage\n",
    "dataset_tokenized = dataset.map(lambda batch: tokenizer(batch['review_body'], padding='max_length', truncation=True, max_length=32))\n",
    "dataset_tokenized = dataset_tokenized.rename_column(\"star_rating\", \"labels\")\n",
    "dataset_tokenized = dataset_tokenized.rename_column(\"review_body\", \"text\")\n",
    "\n",
    "# convert star rating that ranging from 1-5 to labels that ranging from 0-4\n",
    "dataset_tokenized = dataset_tokenized.map(to_label)\n",
    "\n",
    "dataset_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: product_id, review_headline, text, customer_id. If product_id, review_headline, text, customer_id are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 380604\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5947' max='5947' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5947/5947 05:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_output = trainer.predict(dataset_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7102894346880222}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get prediction and evaluate\n",
    "pred = pred_output[0].argmax(axis=1)\n",
    "truth = pred_output[1]\n",
    "accuracy = load_metric('accuracy')\n",
    "f1 = load_metric('f1')\n",
    "accuracy.compute(predictions=pred, references=truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.6851160478312943}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.compute(predictions=pred, references=truth, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dataset expanded by rating predicted by sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the original dataset with the predicted rating\n",
    "data = dataset[:]\n",
    "data['senti_rating_finetune'] = pred + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>senti_rating_finetune</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49033728</td>\n",
       "      <td>6302503213</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17857748</td>\n",
       "      <td>B000059PET</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25551507</td>\n",
       "      <td>0788812807</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21025041</td>\n",
       "      <td>6302509939</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40943563</td>\n",
       "      <td>B00JENS2BI</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user        item  rating  senti_rating_finetune\n",
       "0  49033728  6302503213       5                      5\n",
       "1  17857748  B000059PET       5                      5\n",
       "2  25551507  0788812807       4                      5\n",
       "3  21025041  6302509939       5                      5\n",
       "4  40943563  B00JENS2BI       3                      4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# extract variables needed for CF recommender\n",
    "df= df[['customer_id', 'product_id', 'star_rating', 'senti_rating_finetune']]\n",
    "df.rename(columns = {'customer_id' : 'user', 'product_id' : 'item', 'star_rating' : 'rating'}, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv files\n",
    "df.to_csv('../data/amazon_video_roberta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
